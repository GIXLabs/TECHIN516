# Lab 7: Computer Vision

![terminator_vision](/assets/terminator_vision.png)

Cameras are one of the most common sensors used to connect robots to the real world.  
There are decades of research dedicated to understanding the world from photos and videos.  
In this lab, we will work through the basics of stereo depth and object recognition models.  

## Learning Objectives

- Learn the basics about how robots percieve the world using cameras.
- Learn the basic capabilities of depth cameras.


## TODO

1. Follow the instructions in [this repo](https://github.com/mgonzs13/yolo_ros) to add the yolo_ros package to your workspace.

2. Open the warehouse Gazebo world

3. Read through the yolo_ros page, launch the version with 3D object segmentation that publishes markers for Rviz.  
Note: you will need to remap ros topics from the image topic provided by the OakD camera on the Turtlebot to the topic that the yolo package expects.  

4. Create a new package called `lab7`.  
Write a new python node that subscribes to the `/yolo/detections` and `/oakd/rgb/preview/depth` topics.  
The node should use these topics to find the center-point of a detection, and publish a spherical marker of the object's location in the map frame.  

5. Launch yolo_ros while connected to the real robot.  
Compare depth measurments for 3 different objects from the `/yolo/detections3d` topic compared to measurements taken with a measuring tape.  


## Deliverables

1. What is the license of the repo?  
Are you allowed to use this repo for commercial use?

2. Include a screenshot of an Rviz window that shows the yolo_ros 3D markers highlighting the simulated person, and the pointcloud generated by the simulated depth camera. 

3. What is the ideal depth range for the stereo camera on the Turtlebot 4 Lites?

4. Include a table for your measurements like the following:  

    | objects | yolo detections 3d measurement | measuring tape measurement |
    | - | - | - |
    | object 1 | | |
    | object 2 | | |
    | object 3 | | |


## FAQ

**Q:** How do I find out what the range is of the camera?  
**A:** Refer to the documentation to see what type of camera is used, then look up the documentation for that model.  

**Q:** The yolo image display doesn't show anything! What's wrong?  
**A:** The topic names published by the Turtlebot is different than what yolo_ros subscribes to, you need to provide correct topic names when you run `ros2 launch`.

**Q:** I can't get the markers to display, what do I do?  
**A:** Read through the launch file, find where the scale of the marker is manipulated, and change the parameter when you run `ros2 launch`.  

**Q:** Which objects are recognizable by yolo?  
**A:** Yolo is trained with the [COCO dataset](https://cocodataset.org/#explore) which has a small number of classes that can be recognized.  


## Resources

[Luxonis Docs](https://docs.luxonis.com/software-v3/)  
Luxonis is the company that makes the Oak-D cameras mounted to the Turtlebot 4.  
Checkout their docs for examples of all the features they built in to get ideas on how to use them.

[Official YOLO Documentation](https://docs.ultralytics.com/)  
The yolo vision models are provided and maintained by Ultralytics.  
Check out the official documentation to learn more about how to use them.  

[CpenCV's 2025 Beginner Computer Vision Guide](https://opencv.org/blog/what-is-computer-vision/)  
OpenCV is a popular library for working with visual data.  
It provides efficient functions for common methods in Python and C++.  
In general, the OpenCV organization is a good source for anything computer vision related.  
